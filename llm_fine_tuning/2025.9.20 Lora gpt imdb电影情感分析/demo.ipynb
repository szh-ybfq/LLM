{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e834295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\Basic\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab67f782",
   "metadata": {},
   "source": [
    "### 1、加载并预处理文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e3f03aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_dataset(tokenizer, dataset_path=\"./imdb\", split=\"train\"):\n",
    "    \"\"\"\n",
    "    加载并预处理文本数据集\n",
    "    \n",
    "    参数:\n",
    "        tokenizer: 分词器，用于将文本转换为模型可理解的token\n",
    "        dataset_path: 数据集名称或本地路径，这里使用IMDB电影评论数据集作为示例\n",
    "        split: 要加载的数据集分割部分（train/validation/test）\n",
    "    \n",
    "    返回:\n",
    "        预处理后的tokenized数据集\n",
    "    \"\"\"\n",
    "    # 加载数据集 - 这里使用IMDB情感分析数据集作为示例\n",
    "    # 文件格式转换为json\n",
    "    parquet_path = \"train-00000-of-00001.parquet\"\n",
    "    df = pd.read_parquet(parquet_path, engine=\"pyarrow\")\n",
    "    out_json_path = \"train-00000-of-00001.json\"\n",
    "\n",
    "    df.to_json(\n",
    "        out_json_path,\n",
    "        orient=\"records\", # 每行一个json对象\n",
    "        force_ascii=False, # 保持中文字符\n",
    "        lines=True\n",
    "    )\n",
    "\n",
    "    # 加载数据集        \n",
    "    dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files={\n",
    "            \"train\": \"train-00000-of-00001.json\",\n",
    "        },  split=split\n",
    "    )\n",
    "    print(f\"成功加载数据集，共包含 {len(dataset)} 条样本\")\n",
    "\n",
    "    # 定义预处理函数\n",
    "    def preprocess_function(examples):\n",
    "        \"\"\"\n",
    "        对单批次数据进行预处理：\n",
    "        1. 将文本转换为token\n",
    "        2. 统一序列长度（padding和truncation）\n",
    "        3. 为语言模型任务创建标签\n",
    "        \"\"\"\n",
    "        # 对于文本生成任务，我们将使用\"text\"字段\n",
    "        # 为每条文本添加结束标记，让模型知道生成何时结束\n",
    "        texts = [f\"评论: {text}\\n回复: \" + tokenizer.eos_token for text in examples[\"text\"]]\n",
    "        \n",
    "        # 分词处理\n",
    "        tokenized = tokenizer(\n",
    "            texts,\n",
    "            padding=\"max_length\",  # 填充到最大长度\n",
    "            max_length=128,        # 小模型适合较短的序列长度\n",
    "            truncation=True,       # 截断  过长的文本\n",
    "            return_tensors=\"pt\"    # 返回PyTorch张量\n",
    "        )\n",
    "        \n",
    "        # 对于因果语言模型（如GPT），标签与输入ID相同,所以直接克隆，因为本次的输出会作为下一层的输入\n",
    "        # 因为模型的任务是预测下一个token\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "        \n",
    "        return tokenized\n",
    "    \n",
    "    # 应用预处理函数到整个数据集\n",
    "    # batched=True表示批量处理，加快速度\n",
    "    tokenized_dataset = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,  # 移除原始文本列，只保留处理后的特征\n",
    "        desc=\"正在预处理数据集...\"\n",
    "    )\n",
    "    \n",
    "    return tokenized_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2f8e00",
   "metadata": {},
   "source": [
    "### 2、加载基础模型并配置LoRA适配器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbf1dacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_and_lora(local_model_dir=\"./gpt2\", use_quantization=True):\n",
    "    \"\"\"\n",
    "    加载基础模型并配置LoRA适配器\n",
    "    \n",
    "    参数:\n",
    "        model_name: 预训练模型名称，这里使用小参数的gpt2\n",
    "        use_quantization: 是否使用量化来减少显存占用\n",
    "    \n",
    "    返回:\n",
    "        model: 配置了LoRA的模型\n",
    "        tokenizer: 对应的分词器\n",
    "    \"\"\"\n",
    "    # 加载分词器\n",
    "    tokenizer = AutoTokenizer.from_pretrained(local_model_dir)\n",
    "    \n",
    "    # GPT2默认没有pad_token，需要手动设置\n",
    "    # 这里我们使用eos_token作为pad_token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # 配置量化参数（如果启用）\n",
    "    quantization_config = None\n",
    "    if use_quantization:\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,                # 启用4位量化\n",
    "            bnb_4bit_quant_type=\"nf4\",        # 使用nf4量化类型（适合自然语言）\n",
    "            bnb_4bit_compute_dtype=torch.float16,  # 计算时使用float16\n",
    "            bnb_4bit_use_double_quant=True    # 启用双重量化，进一步减少内存占用\n",
    "        )\n",
    "    \n",
    "    # 加载基础模型\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        local_model_dir,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",  # 自动将模型分配到可用设备\n",
    "        torch_dtype=torch.float16 if use_quantization else None,\n",
    "        local_files_only=True  # 强制只加载本地文件，不尝试在线下载（可选，推荐加）\n",
    "    )\n",
    "    \n",
    "    # 如果使用量化，需要准备模型进行kbit训练\n",
    "    if use_quantization:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    # 配置LoRA参数\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,                      # LoRA的秩，控制低秩矩阵的维度\n",
    "        lora_alpha=32,            # LoRA的缩放因子\n",
    "        target_modules=[\"c_attn\"],# 目标模块，GPT2中注意力模块的名称是c_attn\n",
    "        lora_dropout=0.05,        # Dropout概率，防止过拟合\n",
    "        bias=\"none\",              # 不微调偏置参数\n",
    "        task_type=\"CAUSAL_LM\",    # 任务类型：因果语言模型\n",
    "    )\n",
    "    \n",
    "    # 将LoRA应用到模型\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # 打印可训练参数信息\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b621bac5",
   "metadata": {},
   "source": [
    "### 3、配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "416b3b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_training_args(output_dir=\"./lora_gpt_results\"):\n",
    "    \"\"\"\n",
    "    配置训练参数\n",
    "    \n",
    "    参数:\n",
    "        output_dir: 模型和日志的保存目录\n",
    "    \n",
    "    返回:\n",
    "        TrainingArguments对象，包含所有训练参数\n",
    "    \"\"\"\n",
    "    # 创建输出目录（如果不存在）\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,                # 输出目录\n",
    "        per_device_train_batch_size=64,        # 每个设备的训练批次大小\n",
    "        gradient_accumulation_steps=2,        # 梯度累积步数\n",
    "        learning_rate=2e-4,                   # 学习率，LoRA通常使用比全量微调更大的学习率\n",
    "        num_train_epochs=3,                   # 训练轮数\n",
    "        logging_dir=f\"{output_dir}/logs\",     # 日志目录\n",
    "        logging_steps=10,                     # 每10步打印一次日志\n",
    "        save_steps=100,                       # 每100步保存一次模型\n",
    "        save_total_limit=2,                   # 最多保存2个模型检查点\n",
    "        fp16=True,                            # 启用混合精度训练\n",
    "        report_to=\"tensorboard\",              # 报告日志到TensorBoard\n",
    "        remove_unused_columns=False,          # 不移除未使用的列\n",
    "        optim=\"paged_adamw_8bit\"              # 使用8位优化器，节省内存\n",
    "    )\n",
    "    \n",
    "    return training_args\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c45cce",
   "metadata": {},
   "source": [
    "### 4、训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc2b577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, tokenized_dataset, training_args, tokenizer):\n",
    "    \"\"\"\n",
    "    训练模型\n",
    "    \n",
    "    参数:\n",
    "        model: 要训练的模型\n",
    "        tokenized_dataset: 预处理后的训练数据集\n",
    "        training_args: 训练参数配置\n",
    "    \n",
    "    返回:\n",
    "        训练好的Trainer对象\n",
    "    \"\"\"\n",
    "    # 数据收集器：用于将多个样本组合成批次\n",
    "    # 对于语言模型，我们使用DataCollatorForLanguageModeling\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,  # 对于GPT等因果语言模型，不使用掩码语言模型\n",
    "    )\n",
    "    \n",
    "    # 创建Trainer对象\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    # 开始训练\n",
    "    print(\"开始训练...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # 保存最终模型\n",
    "    trainer.save_model(f\"{training_args.output_dir}/final_model\")\n",
    "    print(f\"模型已保存到 {training_args.output_dir}/final_model\")\n",
    "    \n",
    "    return trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecfc29a",
   "metadata": {},
   "source": [
    "### 5、生成文本函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11b5ca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(model, tokenizer, prompt, max_length=100, temperature=0.7):\n",
    "    \"\"\"\n",
    "    使用训练好的模型生成文本\n",
    "    \n",
    "    参数:\n",
    "        model: 训练好的模型\n",
    "        tokenizer: 分词器\n",
    "        prompt: 生成文本的提示词\n",
    "        max_length: 生成文本的最大长度\n",
    "        temperature: 控制生成文本的随机性（值越小越确定）\n",
    "    \n",
    "    返回:\n",
    "        生成的文本\n",
    "    \"\"\"\n",
    "    # 对提示词进行编码\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\", # 返回PyTorch张量\n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    ).to(model.device) \n",
    "    \n",
    "    # 生成文本\n",
    "    with torch.no_grad():  # 不计算梯度，节省内存\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,  # 启用采样\n",
    "            top_p=0.9,       # 核采样参数\n",
    "            repetition_penalty=1.2,  # 重复惩罚，减少重复生成\n",
    "            pad_token_id=tokenizer.pad_token_id,  # \n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # 解码生成的token为文本\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6f7bbc",
   "metadata": {},
   "source": [
    "### 6、训练流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f12e654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 25000 examples [00:00, 271083.66 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功加载数据集，共包含 25000 条样本\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "正在预处理数据集...: 100%|██████████| 25000/25000 [00:05<00:00, 4203.95 examples/s]\n",
      "The 8-bit optimizer is not available on your device, only available on CUDA for now.\n",
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\Basic\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"d:\\Anaconda\\envs\\Basic\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 772, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"d:\\Anaconda\\envs\\Basic\\lib\\threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\Basic\\lib\\subprocess.py\", line 1495, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "  File \"d:\\Anaconda\\envs\\Basic\\lib\\codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb2 in position 7: invalid start byte\n",
      "d:\\Anaconda\\envs\\Basic\\lib\\site-packages\\peft\\tuners\\lora\\layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.2364\n",
      "开始训练...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\Basic\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m training_args \u001b[38;5;241m=\u001b[39m configure_training_args()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 5. 训练模型（2、3、4）\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 30\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, tokenized_dataset, training_args, tokenizer)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m开始训练...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# 保存最终模型\u001b[39;00m\n\u001b[0;32m     33\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_args\u001b[38;5;241m.\u001b[39moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/final_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Basic\\lib\\site-packages\\transformers\\trainer.py:2328\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2326\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Basic\\lib\\site-packages\\transformers\\trainer.py:2672\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2665\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2666\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2667\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2668\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2669\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2670\u001b[0m )\n\u001b[0;32m   2671\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2672\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2675\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2676\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2677\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2678\u001b[0m ):\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2680\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Basic\\lib\\site-packages\\transformers\\trainer.py:4060\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   4057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   4058\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 4060\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   4062\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Basic\\lib\\site-packages\\accelerate\\accelerator.py:2734\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2733\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2734\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Basic\\lib\\site-packages\\torch\\_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    646\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Basic\\lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Basic\\lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    830\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    831\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 加载分词器（先临时加载用于处理数据）\n",
    "    temp_tokenizer = AutoTokenizer.from_pretrained(\"./gpt2\")\n",
    "    if temp_tokenizer.pad_token is None:\n",
    "        temp_tokenizer.pad_token = temp_tokenizer.eos_token\n",
    "    \n",
    "    # 2. 加载和预处理数据集\n",
    "    tokenized_dataset = load_and_prepare_dataset(temp_tokenizer)\n",
    "    \n",
    "    # 3. 设置模型和LoRA\n",
    "    model, tokenizer = setup_model_and_lora(\n",
    "        local_model_dir=\"./gpt2\",\n",
    "        use_quantization=False  # 小模型也可以使用量化进一步节省内存\n",
    "    )\n",
    "    \n",
    "    # 4. 配置训练参数\n",
    "    training_args = configure_training_args()\n",
    "    \n",
    "    # 5. 训练模型（2、3、4）\n",
    "    trainer = train_model(model, tokenized_dataset, training_args,tokenizer)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50464f4c",
   "metadata": {},
   "source": [
    "## 7、生成文本测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594a256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 测试文本生成\n",
    "test_prompts = [\n",
    "    \"评论: 这部电影真的很好看！特别推荐\\n回复: \",\n",
    "    \"评论: 这是我看过的最糟糕的电影《战狼》，浪费时间。\\n回复: \"\n",
    "]\n",
    "\n",
    "print(\"\\n===== 文本生成测试 =====\")\n",
    "for prompt in test_prompts:\n",
    "    generated = generate_text(model, tokenizer, prompt, max_length=150)\n",
    "    for item in generated.split(\"\\n\"):\n",
    "        # 先处理字符串内部的 \".\"，再打印（自动换行）\n",
    "        processed_item = item.replace(\".\", \".\\n\")\n",
    "        print(processed_item)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
