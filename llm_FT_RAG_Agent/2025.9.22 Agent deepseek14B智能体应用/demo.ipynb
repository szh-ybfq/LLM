{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bafeb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    pipeline,\n",
    "    BitsAndBytesConfig  # ç”¨äºæ¨¡å‹é‡åŒ–ï¼Œé™ä½æ˜¾å­˜å ç”¨\n",
    ")\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fe2539",
   "metadata": {},
   "source": [
    "# 1. GPUé…ç½®æ¨¡å—\n",
    "# --------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "146f0693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup_environment() -> torch.device:\n",
    "    \"\"\"\n",
    "    åˆå§‹åŒ–è¿è¡Œç¯å¢ƒï¼Œä¼˜å…ˆä½¿ç”¨GPUå¹¶æ£€æŸ¥ç¡¬ä»¶é…ç½®\n",
    "    DeepSeek-R1-Distill-Qwen-14Béœ€è¦è‡³å°‘16GBæ˜¾å­˜çš„GPU\n",
    "    \"\"\"\n",
    "    # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨GPU\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        # æ£€æŸ¥GPUæ˜¾å­˜æ˜¯å¦è¶³å¤Ÿ\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024 **3)  # è½¬æ¢ä¸ºGB\n",
    "        if gpu_memory < 16:\n",
    "            print(f\"âš ï¸ è­¦å‘Šï¼šæ£€æµ‹åˆ°GPUæ˜¾å­˜ä¸º{gpu_memory:.1f}GBï¼Œå¯èƒ½ä¸è¶³ä»¥è¿è¡Œæ¨¡å‹ï¼Œå»ºè®®ä½¿ç”¨16GBä»¥ä¸Šæ˜¾å­˜çš„GPU\")\n",
    "        print(f\"âœ… ä½¿ç”¨GPUåŠ é€Ÿï¼š{torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"ğŸ“Š åˆå§‹GPUæ˜¾å­˜å ç”¨ï¼š{torch.cuda.memory_allocated()/1024**2:.2f}MB\")\n",
    "        return device\n",
    "    else:\n",
    "        raise RuntimeError(\"âŒ æœªæ£€æµ‹åˆ°å¯ç”¨GPUï¼ŒDeepSeek-R1-Distill-Qwen-14Béœ€è¦GPUè¿è¡Œ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a0caa6",
   "metadata": {},
   "source": [
    "# 2. LLMæ ¸å¿ƒæ¨¡å—\n",
    "# --------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b47a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_deepseek_llm(device: torch.device) -> tuple[pipeline, AutoTokenizer]:\n",
    "    \"\"\"\n",
    "    åŠ è½½DeepSeek-R1-Distill-Qwen-14Bæ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "    ä½¿ç”¨4bité‡åŒ–å‡å°‘æ˜¾å­˜å ç”¨ï¼ŒåŒæ—¶ä¿æŒè¾ƒå¥½æ€§èƒ½\n",
    "        generation_pipeline: æ–‡æœ¬ç”Ÿæˆç®¡é“\n",
    "    \"\"\"\n",
    "    # æ¨¡å‹åç§°ï¼Œéœ€è¦åœ¨HuggingFaceä¸Šè·å–è®¿é—®æƒé™\n",
    "    model_name = \"./models/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "    \n",
    "    # é…ç½®4bité‡åŒ–å‚æ•°ï¼Œè¿™æ˜¯èƒ½å¤Ÿåœ¨16GBæ˜¾å­˜ä¸­è¿è¡Œ14Bæ¨¡å‹çš„å…³é”®\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,                  # å¯ç”¨4bité‡åŒ–ï¼šINT4ï¼ˆ4ä½æ•´æ•°ï¼‰ï¼Œ0.5å­—èŠ‚\n",
    "        bnb_4bit_use_double_quant=True,     # ä½¿ç”¨åŒé‡åŒ–ï¼Œè¿›ä¸€æ­¥å‡å°‘æ˜¾å­˜å ç”¨\n",
    "        bnb_4bit_quant_type=\"nf4\",          # ä½¿ç”¨å½’ä¸€åŒ–æµ®ç‚¹4bitç±»å‹ï¼Œæ›´é€‚åˆLLM\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16  # è®¡ç®—æ—¶ä½¿ç”¨bfloat16ç²¾åº¦\n",
    "    )\n",
    "    \n",
    "    # åŠ è½½åˆ†è¯å™¨ï¼ŒQwenç³»åˆ—æ¨¡å‹åŸç”Ÿæ”¯æŒpad_token\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"right\"  # è®¾ç½®å³å¡«å……ï¼Œé¿å…å½±å“ç”Ÿæˆç»“æœ\n",
    "    \n",
    "    # åŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨é‡åŒ–é…ç½®å’Œè‡ªåŠ¨è®¾å¤‡æ˜ å°„\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",  # è‡ªåŠ¨å°†æ¨¡å‹å±‚åˆ†é…åˆ°å¯ç”¨è®¾å¤‡\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True  # éœ€è¦ä¿¡ä»»è¿œç¨‹ä»£ç ï¼Œå› ä¸ºæ¨¡å‹ä½¿ç”¨äº†è‡ªå®šä¹‰æ¶æ„\n",
    "    )\n",
    "    \n",
    "    # åˆ›å»ºæ–‡æœ¬ç”Ÿæˆç®¡é“ï¼Œè®¾ç½®é€‚åˆ14Bæ¨¡å‹çš„ç”Ÿæˆå‚æ•°\n",
    "    generation_pipeline = pipeline(\n",
    "        task=\"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=150,        # ç”Ÿæˆå›ç­”çš„æœ€å¤§é•¿åº¦\n",
    "        temperature=0.5,           # æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§ï¼Œ0.5è¡¨ç¤ºä¸­ç­‰éšæœºæ€§\n",
    "        repetition_penalty=1.1,    # é‡å¤æƒ©ç½šï¼Œå‡å°‘é‡å¤ç”Ÿæˆç›¸åŒå†…å®¹\n",
    "        top_p=0.9,                 # æ ¸é‡‡æ ·å‚æ•°ï¼Œæ§åˆ¶ç”Ÿæˆçš„å¤šæ ·æ€§\n",
    "        do_sample=True,            # å¯ç”¨é‡‡æ ·ç”Ÿæˆ\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… DeepSeek-R1-Distill-Qwen-14Bæ¨¡å‹åŠ è½½å®Œæˆï¼ˆ4bité‡åŒ–ï¼‰\")\n",
    "    print(f\"ğŸ“Š æ¨¡å‹åŠ è½½åæ˜¾å­˜å ç”¨ï¼š{torch.cuda.memory_allocated()/1024**2:.2f}MB\")\n",
    "    return generation_pipeline, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a6d168",
   "metadata": {},
   "source": [
    "# 3. å¯¹è¯è®°å¿†æ¨¡å—\n",
    "# --------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e93b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConversationMemory:\n",
    "    \"\"\"\n",
    "    å¯¹è¯è®°å¿†ç®¡ç†æ¨¡å—ï¼Œè´Ÿè´£å­˜å‚¨å’Œç®¡ç†å¯¹è¯å†å²\n",
    "    ç‰¹ç‚¹ï¼š\n",
    "        - æ”¯æŒè¶…é•¿å¯¹è¯å†å²ï¼ˆæœ€é«˜50000Tokenï¼‰\n",
    "        - è‡ªåŠ¨æˆªæ–­è¶…é•¿å†å²ï¼Œä¿ç•™æœ€æ–°å¯¹è¯\n",
    "        - ç”ŸæˆåŒ…å«å†å²ä¸Šä¸‹æ–‡çš„æç¤ºè¯\n",
    "\n",
    "        åˆå§‹åŒ–å¯¹è¯è®°å¿†\n",
    "        å‚æ•°:\n",
    "            tokenizer: ç”¨äºè®¡ç®—Tokené•¿åº¦çš„åˆ†è¯å™¨\n",
    "            max_token_len: æœ€å¤§è®°å¿†é•¿åº¦ï¼ˆTokenæ•°ï¼‰\n",
    "        \"\"\"\n",
    "    def __init__(self, tokenizer: AutoTokenizer, max_token_len: int = 50000):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len  # é€‚é…14Bæ¨¡å‹çš„32Kä¸Šä¸‹æ–‡çª—å£\n",
    "        self.history: List[Dict[str, str]] = []  # å­˜å‚¨å¯¹è¯å†å²\n",
    "\n",
    "    \"\"\"\n",
    "        æ·»åŠ ä¸€è½®å¯¹è¯åˆ°è®°å¿†ä¸­ï¼Œå¹¶åœ¨å¿…è¦æ—¶æˆªæ–­å†å²\n",
    "        \"\"\"\n",
    "    def add_conversation(self, user_query: str, agent_response: str) -> None:\n",
    "        self.history.append({\"user\": user_query, \"agent\": agent_response})\n",
    "        self._truncate_history()  # ç¡®ä¿å†å²ä¸è¶…è¿‡æœ€å¤§é•¿åº¦\n",
    "\n",
    "     \"\"\"\n",
    "        æˆªæ–­è¶…é•¿å¯¹è¯å†å²ï¼Œä»æœ€æ—§çš„å¯¹è¯å¼€å§‹åˆ é™¤\n",
    "        ç¡®ä¿æ€»é•¿åº¦ä¸è¶…è¿‡max_token_len\n",
    "        \"\"\"\n",
    "    def _truncate_history(self) -> None:\n",
    "        while True:\n",
    "            # 1ã€æ‹¼æ¥æ‰€æœ‰å†å²å¯¹è¯ä¸ºæ–‡æœ¬\n",
    "            history_text = \"\\n\".join([\n",
    "                f\"ç”¨æˆ·ï¼š{item['user']}\\nåŠ©æ‰‹ï¼š{item['agent']}\" \n",
    "                for item in self.history\n",
    "            ])\n",
    "            \n",
    "            # 2ã€è®¡ç®—å½“å‰å†å²çš„Tokené•¿åº¦\n",
    "            token_len = len(self.tokenizer.encode(history_text, add_special_tokens=False))\n",
    "            \n",
    "            # 3ã€å¦‚æœé•¿åº¦åœ¨é™åˆ¶èŒƒå›´å†…ï¼Œæˆ–åªå‰©æœ€åä¸€è½®å¯¹è¯ï¼Œåˆ™åœæ­¢æˆªæ–­\n",
    "            if token_len <= self.max_token_len or len(self.history) <= 1:\n",
    "                break\n",
    "            \n",
    "            # 4ã€åˆ é™¤æœ€æ—§çš„ä¸€è½®å¯¹è¯\n",
    "            self.history.pop(0)\n",
    "        \n",
    "        # æ‰“å°å½“å‰è®°å¿†é•¿åº¦ï¼ˆè°ƒè¯•ç”¨ï¼‰\n",
    "        # current_len = len(self.tokenizer.encode(\n",
    "        #     \"\\n\".join([f\"ç”¨æˆ·ï¼š{i['user']}\\nåŠ©æ‰‹ï¼š{i['agent']}\" for i in self.history]),\n",
    "        #     add_special_tokens=False\n",
    "        # ))\n",
    "        # print(f\"ğŸ’¾ å½“å‰å¯¹è¯è®°å¿†é•¿åº¦ï¼š{current_len} Token\")\n",
    "        \n",
    "    \"\"\"\n",
    "        ç”ŸæˆåŒ…å«å†å²å¯¹è¯çš„æç¤ºè¯ï¼Œç”¨äºæ¨¡å‹ç”Ÿæˆå›ç­”\n",
    "        å‚æ•°:\n",
    "            current_query: å½“å‰ç”¨æˆ·æŸ¥è¯¢\n",
    "        è¿”å›:\n",
    "            åŒ…å«å†å²ä¸Šä¸‹æ–‡çš„æç¤ºè¯\n",
    "        \"\"\"\n",
    "    def get_context_prompt(self, current_query: str) -> str:\n",
    "        \n",
    "        # å¦‚æœæ²¡æœ‰å†å²å¯¹è¯ï¼Œç›´æ¥è¿”å›å½“å‰é—®é¢˜\n",
    "        if not self.history:\n",
    "            return f\"ç”¨æˆ·é—®ï¼š{current_query}\\nåŠ©æ‰‹å›ç­”ï¼š\"\n",
    "        \n",
    "        # æ‹¼æ¥å†å²å¯¹è¯å’Œå½“å‰é—®é¢˜\n",
    "        history_text = \"\\n\".join([\n",
    "            f\"ç”¨æˆ·ï¼š{item['user']}\\nåŠ©æ‰‹ï¼š{item['agent']}\" \n",
    "            for item in self.history\n",
    "        ])\n",
    "        \n",
    "        # ç”Ÿæˆæç¤ºè¯ï¼ŒæŒ‡å¯¼æ¨¡å‹åŸºäºå†å²å’Œå½“å‰é—®é¢˜ç”Ÿæˆå›ç­”\n",
    "        return f\"\"\"åŸºäºä»¥ä¸‹å†å²å¯¹è¯å’Œå½“å‰é—®é¢˜ï¼Œç”Ÿæˆå‡†ç¡®ã€ç®€æ´ã€æœ‰ç”¨çš„å›ç­”ï¼š\n",
    "{history_text}\n",
    "ç”¨æˆ·ç°åœ¨é—®ï¼š{current_query}\n",
    "åŠ©æ‰‹å›ç­”ï¼š\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3926605b",
   "metadata": {},
   "source": [
    "# 4. å†³ç­–æ¨¡å—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe9ff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def agent_decision(llm_pipeline: pipeline, query: str, memory: ConversationMemory) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Agentå†³ç­–é€»è¾‘ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·ï¼Œä»¥åŠè°ƒç”¨å“ªä¸ªå·¥å…·\n",
    "    \n",
    "    å‚æ•°:\n",
    "        llm_pipeline: æ–‡æœ¬ç”Ÿæˆç®¡é“\n",
    "        query: å½“å‰ç”¨æˆ·æŸ¥è¯¢\n",
    "        \n",
    "    è¿”å›:\n",
    "        å†³ç­–ç»“æœå­—å…¸ï¼šåŒ…å«actionå’Œå¯é€‰çš„tool_name\n",
    "    \"\"\"\n",
    "    # ç”Ÿæˆå†³ç­–æç¤ºè¯\n",
    "    decision_prompt = f\"\"\"\n",
    "ä»»åŠ¡ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼Œå¹¶é€‰æ‹©åˆé€‚çš„å·¥å…·ã€‚\n",
    "\n",
    "åˆ¤æ–­æ ‡å‡†ï¼š\n",
    "1. å¦‚æœæ˜¯æ•°å­¦è®¡ç®—é—®é¢˜ï¼ˆåŒ…å«æ•°å­—å’Œè¿ç®—ç¬¦ï¼‰â†’ è°ƒç”¨è®¡ç®—å™¨å·¥å…·ï¼Œè¾“å‡ºï¼štool:calculator\n",
    "2. å¦‚æœæ˜¯è¯¢é—®æœ¬Agentçš„åŠŸèƒ½ã€ä½¿ç”¨æ–¹æ³• â†’ è°ƒç”¨æ–‡æ¡£æ£€ç´¢å·¥å…·ï¼Œè¾“å‡ºï¼štool:document_retrieval\n",
    "3. å¦‚æœæ˜¯éœ€è¦å®æ—¶ä¿¡æ¯çš„é—®é¢˜ï¼ˆå¦‚å¤©æ°”ã€æ–°é—»ã€å®æ—¶æ•°æ®ç­‰ï¼‰â†’ è°ƒç”¨æœç´¢å·¥å…·ï¼Œè¾“å‡ºï¼štool:search\n",
    "4. å…¶ä»–æƒ…å†µ â†’ ç›´æ¥å›ç­”ï¼Œè¾“å‡ºï¼šdirect\n",
    "\n",
    "éœ€è¦åˆ†æçš„é—®é¢˜ï¼š{query}\n",
    "\n",
    "å‚è€ƒå†å²å¯¹è¯ï¼ˆæœ€è¿‘2è½®ï¼‰ï¼š{memory.history[-2:] if len(memory.history)>=2 else \"æ— \"}\n",
    "\n",
    "è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼è¾“å‡ºå†³ç­–ç»“æœï¼Œä»…è¾“å‡ºå†³ç­–å†…å®¹ï¼Œä¸æ·»åŠ é¢å¤–ä¿¡æ¯ã€‚\n",
    "å†³ç­–ï¼š\n",
    "\"\"\"\n",
    "    \n",
    "    # è°ƒç”¨æ¨¡å‹ç”Ÿæˆå†³ç­–\n",
    "    decision_result = llm_pipeline(decision_prompt)[0][\"generated_text\"]\n",
    "    # æå–å†³ç­–ç»“æœï¼ˆå–\"å†³ç­–ï¼š\"åé¢çš„å†…å®¹ï¼‰\n",
    "    decision_result = decision_result.split(\"å†³ç­–ï¼š\")[-1].strip().lower()\n",
    "    \n",
    "    # è§£æå†³ç­–ç»“æœ\n",
    "    if \"tool:calculator\" in decision_result:\n",
    "        return {\"action\": \"tool\", \"tool_name\": \"calculator\"}\n",
    "    elif \"tool:document_retrieval\" in decision_result:\n",
    "        return {\"action\": \"tool\", \"tool_name\": \"document_retrieval\"}\n",
    "    elif \"tool:search\" in decision_result:\n",
    "        return {\"action\": \"tool\", \"tool_name\": \"search\"}\n",
    "    else:\n",
    "        # å¦‚æœå†³ç­–ä¸æ˜ç¡®ï¼Œé»˜è®¤ç›´æ¥å›ç­”\n",
    "        return {\"action\": \"direct\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dff75f",
   "metadata": {},
   "source": [
    "# 5. å·¥å…·æ¨¡å—\n",
    "# --------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0ce6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ToolManager:\n",
    "    \"\"\"\n",
    "    å·¥å…·ç®¡ç†æ¨¡å—ï¼Œå®šä¹‰Agentå¯ä»¥è°ƒç”¨çš„å·¥å…·\n",
    "    \n",
    "    ç›®å‰æ”¯æŒçš„å·¥å…·ï¼š\n",
    "    - è®¡ç®—å™¨ï¼šå¤„ç†æ•°å­¦è®¡ç®—\n",
    "    - æ–‡æ¡£æ£€ç´¢ï¼šä»æœ¬åœ°æ–‡æ¡£ä¸­æŸ¥æ‰¾ä¿¡æ¯\n",
    "    - æ¨¡æ‹Ÿæœç´¢ï¼šæ¨¡æ‹Ÿç½‘ç»œæœç´¢åŠŸèƒ½\n",
    "    \"\"\"\n",
    "    def __init__(self, device: torch.device, doc_path: str = \"agent_knowledge.txt\"):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–å·¥å…·ç®¡ç†å™¨\n",
    "        \n",
    "        å‚æ•°:\n",
    "            device: è¿è¡Œè®¾å¤‡\n",
    "            doc_path: æ–‡æ¡£æ£€ç´¢çš„çŸ¥è¯†åº“è·¯å¾„\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        \n",
    "        # å·¥å…·æ³¨å†Œè¡¨ï¼Œé”®ä¸ºå·¥å…·åç§°ï¼Œå€¼ä¸ºå·¥å…·å‡½æ•°\n",
    "        self.tools = {\n",
    "            \"calculator\": self._calculator_tool,\n",
    "            \"document_retrieval\": self._document_retrieval_tool,\n",
    "            \"search\": self._simulate_search_tool\n",
    "        }\n",
    "        \n",
    "        # åˆå§‹åŒ–æ–‡æ¡£æ£€ç´¢å·¥å…·\n",
    "        self._init_document_retrieval(doc_path)\n",
    "        print(\"âœ… å·¥å…·æ¨¡å—åˆå§‹åŒ–å®Œæˆï¼ˆæ”¯æŒè®¡ç®—å™¨ã€æ–‡æ¡£æ£€ç´¢ã€æ¨¡æ‹Ÿæœç´¢ï¼‰\")\n",
    "    \n",
    "    def _init_document_retrieval(self, doc_path: str) -> None:\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–æ–‡æ¡£æ£€ç´¢å·¥å…·ï¼ˆRAGï¼šæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰\n",
    "        \n",
    "        æ­¥éª¤ï¼š\n",
    "        1. ç”Ÿæˆæ ·æœ¬æ–‡æ¡£ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰\n",
    "        2. åŠ è½½æ–‡æ¡£å¹¶åˆ†å‰²ä¸ºå°å—\n",
    "        3. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹\n",
    "        4. åˆ›å»ºå‘é‡æ•°æ®åº“å­˜å‚¨æ–‡æ¡£å‘é‡\n",
    "        \"\"\"\n",
    "        # ç”Ÿæˆæ ·æœ¬æ–‡æ¡£ï¼ˆAgentçš„çŸ¥è¯†åº“ï¼‰\n",
    "        if not os.path.exists(doc_path):\n",
    "            with open(doc_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"æœ¬AgentåŸºäºDeepSeek-R1-Distill-Qwen-14Bå¤§è¯­è¨€æ¨¡å‹å¼€å‘ï¼Œå…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š\\n\")\n",
    "                f.write(\"1. è‡ªç„¶è¯­è¨€å¯¹è¯ï¼šå¯ä»¥è¿›è¡Œå¤šè½®å¯¹è¯ï¼Œç†è§£ä¸Šä¸‹æ–‡è¯­å¢ƒ\\n\")\n",
    "                f.write(\"2. æ•°å­¦è®¡ç®—ï¼šèƒ½å¤Ÿå¤„ç†åŠ å‡ä¹˜é™¤ç­‰åŸºæœ¬è¿ç®—ï¼Œæ”¯æŒæ‹¬å·å’Œå°æ•°\\n\")\n",
    "                f.write(\"3. æ–‡æ¡£æ£€ç´¢ï¼šå¯ä»¥å›ç­”å…³äºè‡ªèº«åŠŸèƒ½å’Œä½¿ç”¨æ–¹æ³•çš„é—®é¢˜\\n\")\n",
    "                f.write(\"4. æ¨¡æ‹Ÿæœç´¢ï¼šå¯¹äºéœ€è¦å®æ—¶ä¿¡æ¯çš„é—®é¢˜ï¼Œä¼šæä¾›æ¨¡æ‹Ÿæœç´¢ç»“æœ\\n\")\n",
    "                f.write(\"ä½¿ç”¨æ–¹æ³•ï¼šç›´æ¥è¾“å…¥æ‚¨çš„é—®é¢˜å³å¯ï¼ŒAgentä¼šè‡ªåŠ¨åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·æ¥å›ç­”\\n\")\n",
    "        \n",
    "        # 1ã€åŠ è½½æ–‡æ¡£\n",
    "        loader = TextLoader(doc_path, encoding=\"utf-8\")\n",
    "        documents = loader.load()\n",
    "\n",
    "        # å°†éå­—ç¬¦ä¸²å†…å®¹è½¬æ¢ä¸ºå­—ç¬¦ä¸²\n",
    "        for i, doc in enumerate(documents):\n",
    "            if not isinstance(doc.page_content, str):\n",
    "                documents[i].page_content = str(doc.page_content)\n",
    "                print(f\"âš ï¸  ä¿®å¤æ–‡æ¡£ {i} å†…å®¹ç±»å‹ï¼šéå­—ç¬¦ä¸²â†’å­—ç¬¦ä¸²\")\n",
    "        # 2ã€åˆ†å‰²æ–‡æ¡£ä¸ºé€‚åˆåµŒå…¥çš„å°å—ï¼Œ     é€’å½’åˆ†å‰²\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=100,      # æ¯ä¸ªæ–‡æœ¬å—çš„å¤§å°\n",
    "            chunk_overlap=30,    # æ–‡æœ¬å—ä¹‹é—´çš„é‡å éƒ¨åˆ†\n",
    "            separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]  # ä¸­æ–‡åˆ†å‰²ç¬¦ä¼˜åŒ–\n",
    "        )\n",
    "        split_docs = text_splitter.split_documents(documents)\n",
    "        \n",
    "        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ï¼‰\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"all-MiniLM-L6-v2\",  # è½»é‡çº§åµŒå…¥æ¨¡å‹\n",
    "            model_kwargs={\"device\": self.device.type}  # ä½¿ç”¨GPUåŠ é€Ÿ\n",
    "        )\n",
    "        \n",
    "        # åˆå§‹åŒ–Chromaå‘é‡æ•°æ®åº“ï¼Œå­˜å‚¨æ–‡æ¡£å‘é‡\n",
    "        self.vector_db = Chroma.from_documents(\n",
    "            documents=split_docs,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=\"./deepseek_agent_chroma_db\"  # å‘é‡æ•°æ®æŒä¹…åŒ–è·¯å¾„\n",
    "        )\n",
    "        self.vector_db.persist()  # ä¿å­˜å‘é‡æ•°æ®åº“\n",
    "    \n",
    "    def _calculator_tool(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        è®¡ç®—å™¨å·¥å…·ï¼šå¤„ç†æ•°å­¦è®¡ç®—\n",
    "        \n",
    "        å‚æ•°:\n",
    "            query: åŒ…å«æ•°å­¦è¡¨è¾¾å¼çš„æŸ¥è¯¢\n",
    "            \n",
    "        è¿”å›:\n",
    "            è®¡ç®—ç»“æœæˆ–é”™è¯¯ä¿¡æ¯\n",
    "        \"\"\"\n",
    "        # æå–æŸ¥è¯¢ä¸­çš„æ•°å­¦è¡¨è¾¾å¼\n",
    "        expr_match = re.search(r\"[\\d\\(\\)\\+\\-\\Ã—\\*\\/\\.\\s]+\", query)\n",
    "        if not expr_match:\n",
    "            return \"æœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„æ•°å­¦è¡¨è¾¾å¼ï¼Œè¯·è¾“å…¥åŒ…å«æ•°å­—å’Œè¿ç®—ç¬¦(+ã€-ã€*ã€/ã€Ã—ã€Ã·)çš„é—®é¢˜\"\n",
    "        \n",
    "        # æå–å¹¶æ¸…ç†è¡¨è¾¾å¼\n",
    "        expr = expr_match.group(0).strip()\n",
    "        expr = expr.replace(\"Ã—\", \"*\").replace(\"Ã·\", \"/\")  # ç»Ÿä¸€è¿ç®—ç¬¦\n",
    "        \n",
    "        # å®‰å…¨æ£€æŸ¥ï¼šåªå…è®¸æ•°å­—ã€è¿ç®—ç¬¦å’Œæ‹¬å·\n",
    "        safe_chars = set(\"0123456789.+-*/() \")\n",
    "        if not all(c in safe_chars for c in expr):\n",
    "            return \"è¡¨è¾¾å¼åŒ…å«ä¸å®‰å…¨å­—ç¬¦ï¼Œä»…æ”¯æŒæ•°å­—å’ŒåŸºæœ¬è¿ç®—ç¬¦(+ã€-ã€*ã€/ã€()ã€.)\"\n",
    "        \n",
    "        # è®¡ç®—è¡¨è¾¾å¼\n",
    "        try:\n",
    "            result = eval(expr)  # ä½¿ç”¨evalè®¡ç®—è¡¨è¾¾å¼\n",
    "            return f\"è®¡ç®—ç»“æœï¼š{expr} = {float(result):.4g}\"\n",
    "        except ZeroDivisionError:\n",
    "            return \"è®¡ç®—é”™è¯¯ï¼šé™¤æ•°ä¸èƒ½ä¸º0\"\n",
    "        except Exception as e:\n",
    "            return f\"è®¡ç®—é”™è¯¯ï¼šæ— æ³•è®¡ç®—è¡¨è¾¾å¼ '{expr}'ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{str(e)}\"\n",
    "    \n",
    "    def _document_retrieval_tool(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        æ–‡æ¡£æ£€ç´¢å·¥å…·ï¼šä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯\n",
    "        \n",
    "        å‚æ•°:\n",
    "            query: æ£€ç´¢æŸ¥è¯¢\n",
    "            \n",
    "        è¿”å›:\n",
    "            æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯\n",
    "        \"\"\"\n",
    "        # ä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢æœ€ç›¸å…³çš„3ä¸ªæ–‡æ¡£ç‰‡æ®µ\n",
    "        retriever = self.vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "        relevant_docs = retriever.get_relevant_documents(query)\n",
    "        \n",
    "        if not relevant_docs:\n",
    "            return \"æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£ä¿¡æ¯\"\n",
    "        \n",
    "        # æ‹¼æ¥æ£€ç´¢ç»“æœ\n",
    "        context = \"\\n\".join([\n",
    "            f\"[ç›¸å…³ç‰‡æ®µ{i+1}] {doc.page_content}\" \n",
    "            for i, doc in enumerate(relevant_docs)\n",
    "        ])\n",
    "        return f\"ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°ä»¥ä¸‹ç›¸å…³ä¿¡æ¯ï¼š\\n{context}\"\n",
    "    \n",
    "    def _simulate_search_tool(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        æ¨¡æ‹Ÿæœç´¢å·¥å…·ï¼šæ¨¡æ‹Ÿç½‘ç»œæœç´¢åŠŸèƒ½\n",
    "        \n",
    "        å‚æ•°:\n",
    "            query: æœç´¢æŸ¥è¯¢\n",
    "            \n",
    "        è¿”å›:\n",
    "            æ¨¡æ‹Ÿçš„æœç´¢ç»“æœ\n",
    "        \"\"\"\n",
    "        return f\"æ¨¡æ‹Ÿæœç´¢ç»“æœï¼ˆéå®æ—¶æ•°æ®ï¼‰ï¼šå…³äº'{query}'çš„ä¿¡æ¯\\n\" \\\n",
    "               f\"1. åŸºæœ¬æ¦‚è¿°ï¼šè¿™æ˜¯ä¸€ä¸ªå…³äº'{query}'çš„ç¤ºä¾‹æœç´¢ç»“æœ\\n\" \\\n",
    "               f\"2. æç¤ºï¼šè¦è·å–æœ€æ–°ä¿¡æ¯ï¼Œè¯·ä½¿ç”¨æµè§ˆå™¨è®¿é—®æœç´¢å¼•æ“\"\n",
    "    \n",
    "    def call_tool(self, tool_name: str, query: str) -> str:\n",
    "        \"\"\"\n",
    "        è°ƒç”¨æŒ‡å®šå·¥å…·\n",
    "        \n",
    "        å‚æ•°:\n",
    "            tool_name: å·¥å…·åç§°\n",
    "            query: å·¥å…·è¾“å…¥æŸ¥è¯¢\n",
    "            \n",
    "        è¿”å›:\n",
    "            å·¥å…·æ‰§è¡Œç»“æœ\n",
    "        \"\"\"\n",
    "        if tool_name not in self.tools:\n",
    "            return f\"ä¸æ”¯æŒçš„å·¥å…·ï¼š{tool_name}ï¼Œå½“å‰æ”¯æŒçš„å·¥å…·åŒ…æ‹¬ï¼š{list(self.tools.keys())}\"\n",
    "        \n",
    "        try:\n",
    "            return self.tools[tool_name](query)\n",
    "        except Exception as e:\n",
    "            return f\"å·¥å…·è°ƒç”¨å‡ºé”™ï¼š{str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f61d33c",
   "metadata": {},
   "source": [
    "# 6. æ‰§è¡Œæ¨¡å—\n",
    "# --------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05de774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DeepSeekAgentAssistant:\n",
    "    \"\"\"\n",
    "    åŸºäºDeepSeek-R1-Distill-Qwen-14Bçš„AgentåŠ©æ‰‹\n",
    "    \n",
    "    æ ¸å¿ƒåŠŸèƒ½ï¼š\n",
    "    - æ¥æ”¶ç”¨æˆ·è¾“å…¥\n",
    "    - å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·\n",
    "    - æ‰§è¡Œå†³ç­–ï¼ˆè°ƒç”¨å·¥å…·æˆ–ç›´æ¥å›ç­”ï¼‰\n",
    "    - å­˜å‚¨å¯¹è¯å†å²\n",
    "    - è¿”å›æœ€ç»ˆå›ç­”\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"åˆå§‹åŒ–Agentçš„æ‰€æœ‰ç»„ä»¶\"\"\"\n",
    "        # 1. åˆå§‹åŒ–è¿è¡Œç¯å¢ƒ\n",
    "        self.device = setup_environment()\n",
    "        \n",
    "        # 2. åŠ è½½LLMæ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "        self.llm_pipeline, self.tokenizer = load_deepseek_llm(self.device)\n",
    "        \n",
    "        # 3. åˆå§‹åŒ–å¯¹è¯è®°å¿†\n",
    "        self.memory = ConversationMemory(self.tokenizer)\n",
    "        \n",
    "        # 4. åˆå§‹åŒ–å·¥å…·ç®¡ç†å™¨\n",
    "        self.tool_manager = ToolManager(self.device)\n",
    "        \n",
    "        print(\"ğŸ‰ DeepSeek-R1-Distill-Qwen-14B AgentåŠ©æ‰‹åˆå§‹åŒ–å®Œæˆï¼Œ ready to use!\")\n",
    "    \n",
    "    def run(self, user_query: str) -> str:\n",
    "        \"\"\"\n",
    "        æ‰§è¡ŒAgentçš„å®Œæ•´å·¥ä½œæµç¨‹\n",
    "        \n",
    "        å‚æ•°:\n",
    "            user_query: ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢\n",
    "            \n",
    "        è¿”å›:\n",
    "            Agentç”Ÿæˆçš„å›ç­”\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ“¥ ç”¨æˆ·è¾“å…¥ï¼š{user_query}\")\n",
    "        \n",
    "        # æ­¥éª¤1ï¼šAgentå†³ç­–ï¼ˆæ˜¯å¦è°ƒç”¨å·¥å…·ï¼‰\n",
    "        decision = agent_decision(self.llm_pipeline, user_query, self.memory)\n",
    "        print(f\"ğŸ” Agentå†³ç­–ï¼š{decision}\")\n",
    "        \n",
    "        # æ­¥éª¤2ï¼šæ‰§è¡Œå†³ç­–\n",
    "        try:\n",
    "            if decision[\"action\"] == \"tool\":\n",
    "                # è°ƒç”¨å·¥å…·å¹¶è·å–ç»“æœ\n",
    "                tool_name = decision[\"tool_name\"]\n",
    "                tool_result = self.tool_manager.call_tool(tool_name, user_query)\n",
    "                print(f\"ğŸ› ï¸  å·¥å…·æ‰§è¡Œç»“æœï¼š{tool_result[:200]}...\" if len(tool_result) > 200 else f\"ğŸ› ï¸  å·¥å…·æ‰§è¡Œç»“æœï¼š{tool_result}\")\n",
    "                \n",
    "                # ç»“åˆå·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›ç­”\n",
    "                final_prompt = self.memory.get_context_prompt(user_query)\n",
    "                final_prompt += f\"è¯·æ ¹æ®ä»¥ä¸‹å·¥å…·ç»“æœå›ç­”é—®é¢˜ï¼š{tool_result}\"\n",
    "                agent_response = self.llm_pipeline(final_prompt)[0][\"generated_text\"]\n",
    "                \n",
    "            else:\n",
    "                # ç›´æ¥ç”Ÿæˆå›ç­”ï¼ˆä¸è°ƒç”¨å·¥å…·ï¼‰\n",
    "                final_prompt = self.memory.get_context_prompt(user_query)\n",
    "                agent_response = self.llm_pipeline(final_prompt)[0][\"generated_text\"]\n",
    "            \n",
    "            # æå–å¹¶æ¸…ç†å›ç­”ï¼ˆå»æ‰æç¤ºè¯éƒ¨åˆ†å’Œç‰¹æ®Šæ ‡è®°ï¼‰\n",
    "            agent_response = agent_response.split(\"åŠ©æ‰‹å›ç­”ï¼š\")[-1].strip().replace(\"</s>\", \"\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            # å¤„ç†å¯èƒ½çš„é”™è¯¯\n",
    "            agent_response = f\"å¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºé”™ï¼š{str(e)}\"\n",
    "            print(f\"âŒ æ‰§è¡Œé”™è¯¯ï¼š{str(e)}\")\n",
    "        \n",
    "        # æ­¥éª¤3ï¼šå°†æœ¬è½®å¯¹è¯æ·»åŠ åˆ°è®°å¿†\n",
    "        self.memory.add_conversation(user_query, agent_response)\n",
    "        \n",
    "        print(f\"ğŸ“¤ Agentå›ç­”ï¼š{agent_response}\")\n",
    "        return agent_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e6785e",
   "metadata": {},
   "source": [
    "# 7. æµ‹è¯•æ¨¡å—ï¼ˆäº¤äº’å…¥å£ï¼‰\n",
    "# --------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd2622f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ æ³¨æ„ï¼šä½¿ç”¨å‰è¯·ç¡®ä¿å·²åœ¨HuggingFaceè·å–æ¨¡å‹è®¿é—®æƒé™ï¼Œå¹¶é€šè¿‡`huggingface-cli login`å‘½ä»¤ç™»å½•\n",
      "åˆå§‹åŒ–å¤±è´¥ï¼šâŒ æœªæ£€æµ‹åˆ°å¯ç”¨GPUï¼ŒDeepSeek-R1-Distill-Qwen-14Béœ€è¦GPUè¿è¡Œ\n",
      "ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š\n",
      "1. ç¡®ä¿æ‚¨çš„GPUæ˜¾å­˜ä¸å°‘äº16GB\n",
      "2. ç¡®ä¿å·²å®‰è£…å¿…è¦ä¾èµ–ï¼špip install torch transformers bitsandbytes langchain-huggingface chromadb sentence-transformers\n",
      "3. ç¡®ä¿å·²è·å–æ¨¡å‹è®¿é—®æƒé™å¹¶æˆåŠŸç™»å½•\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # æç¤ºç”¨æˆ·éœ€è¦æ¨¡å‹è®¿é—®æƒé™\n",
    "    print(\"âš ï¸ æ³¨æ„ï¼šä½¿ç”¨å‰è¯·ç¡®ä¿å·²åœ¨HuggingFaceè·å–æ¨¡å‹è®¿é—®æƒé™ï¼Œå¹¶é€šè¿‡`huggingface-cli login`å‘½ä»¤ç™»å½•\")\n",
    "    \n",
    "    try:\n",
    "        # åˆå§‹åŒ–Agent\n",
    "        agent = DeepSeekAgentAssistant()\n",
    "        \n",
    "        # å¯åŠ¨äº¤äº’å¾ªç¯\n",
    "        print(\"\\n===== DeepSeek-R1-Distill-Qwen-14B AgentåŠ©æ‰‹ =====\")\n",
    "        print(\"æç¤ºï¼šè¾“å…¥'é€€å‡º'å¯ç»“æŸå¯¹è¯\")\n",
    "        \n",
    "        while True:\n",
    "            user_input = input(\"ä½ ï¼š\")\n",
    "            # æ£€æŸ¥æ˜¯å¦é€€å‡º\n",
    "            if user_input.strip().lower() in [\"é€€å‡º\", \"quit\", \"exit\"]:\n",
    "                print(\"Agentï¼šå†è§ï¼æœ‰ä»»ä½•é—®é¢˜æ¬¢è¿å†æ¬¡å’¨è¯¢ï½\")\n",
    "                break\n",
    "            \n",
    "            # å¤„ç†ç”¨æˆ·è¾“å…¥å¹¶è·å–å›ç­”\n",
    "            response = agent.run(user_input)\n",
    "            print(f\"Agentï¼š{response}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"åˆå§‹åŒ–å¤±è´¥ï¼š{str(e)}\")\n",
    "        print(\"ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š\")\n",
    "        print(\"1. ç¡®ä¿æ‚¨çš„GPUæ˜¾å­˜ä¸å°‘äº16GB\")\n",
    "        print(\"2. ç¡®ä¿å·²å®‰è£…å¿…è¦ä¾èµ–ï¼špip install torch transformers bitsandbytes langchain-huggingface chromadb sentence-transformers\")\n",
    "        print(\"3. ç¡®ä¿å·²è·å–æ¨¡å‹è®¿é—®æƒé™å¹¶æˆåŠŸç™»å½•\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9a7f74",
   "metadata": {},
   "source": [
    "# è¾“å‡º"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
