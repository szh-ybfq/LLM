{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bafeb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    pipeline,\n",
    "    BitsAndBytesConfig  # 用于模型量化，降低显存占用\n",
    ")\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fe2539",
   "metadata": {},
   "source": [
    "# 1. GPU配置模块\n",
    "# --------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "146f0693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup_environment() -> torch.device:\n",
    "    \"\"\"\n",
    "    初始化运行环境，优先使用GPU并检查硬件配置\n",
    "    DeepSeek-R1-Distill-Qwen-14B需要至少16GB显存的GPU\n",
    "    \"\"\"\n",
    "    # 检查是否有可用GPU\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        # 检查GPU显存是否足够\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024 **3)  # 转换为GB\n",
    "        if gpu_memory < 16:\n",
    "            print(f\"⚠️ 警告：检测到GPU显存为{gpu_memory:.1f}GB，可能不足以运行模型，建议使用16GB以上显存的GPU\")\n",
    "        print(f\"✅ 使用GPU加速：{torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"📊 初始GPU显存占用：{torch.cuda.memory_allocated()/1024**2:.2f}MB\")\n",
    "        return device\n",
    "    else:\n",
    "        raise RuntimeError(\"❌ 未检测到可用GPU，DeepSeek-R1-Distill-Qwen-14B需要GPU运行\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a0caa6",
   "metadata": {},
   "source": [
    "# 2. LLM核心模块\n",
    "# --------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b47a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_deepseek_llm(device: torch.device) -> tuple[pipeline, AutoTokenizer]:\n",
    "    \"\"\"\n",
    "    加载DeepSeek-R1-Distill-Qwen-14B模型和分词器\n",
    "    使用4bit量化减少显存占用，同时保持较好性能\n",
    "        generation_pipeline: 文本生成管道\n",
    "    \"\"\"\n",
    "    # 模型名称，需要在HuggingFace上获取访问权限\n",
    "    model_name = \"./models/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "    \n",
    "    # 配置4bit量化参数，这是能够在16GB显存中运行14B模型的关键\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,                  # 启用4bit量化：INT4（4位整数），0.5字节\n",
    "        bnb_4bit_use_double_quant=True,     # 使用双量化，进一步减少显存占用\n",
    "        bnb_4bit_quant_type=\"nf4\",          # 使用归一化浮点4bit类型，更适合LLM\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16  # 计算时使用bfloat16精度\n",
    "    )\n",
    "    \n",
    "    # 加载分词器，Qwen系列模型原生支持pad_token\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"right\"  # 设置右填充，避免影响生成结果\n",
    "    \n",
    "    # 加载模型，使用量化配置和自动设备映射\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",  # 自动将模型层分配到可用设备\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True  # 需要信任远程代码，因为模型使用了自定义架构\n",
    "    )\n",
    "    \n",
    "    # 创建文本生成管道，设置适合14B模型的生成参数\n",
    "    generation_pipeline = pipeline(\n",
    "        task=\"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=150,        # 生成回答的最大长度\n",
    "        temperature=0.5,           # 控制生成的随机性，0.5表示中等随机性\n",
    "        repetition_penalty=1.1,    # 重复惩罚，减少重复生成相同内容\n",
    "        top_p=0.9,                 # 核采样参数，控制生成的多样性\n",
    "        do_sample=True,            # 启用采样生成\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    print(\"✅ DeepSeek-R1-Distill-Qwen-14B模型加载完成（4bit量化）\")\n",
    "    print(f\"📊 模型加载后显存占用：{torch.cuda.memory_allocated()/1024**2:.2f}MB\")\n",
    "    return generation_pipeline, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a6d168",
   "metadata": {},
   "source": [
    "# 3. 对话记忆模块\n",
    "# --------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e93b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConversationMemory:\n",
    "    \"\"\"\n",
    "    对话记忆管理模块，负责存储和管理对话历史\n",
    "    特点：\n",
    "        - 支持超长对话历史（最高50000Token）\n",
    "        - 自动截断超长历史，保留最新对话\n",
    "        - 生成包含历史上下文的提示词\n",
    "\n",
    "        初始化对话记忆\n",
    "        参数:\n",
    "            tokenizer: 用于计算Token长度的分词器\n",
    "            max_token_len: 最大记忆长度（Token数）\n",
    "        \"\"\"\n",
    "    def __init__(self, tokenizer: AutoTokenizer, max_token_len: int = 50000):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len  # 适配14B模型的32K上下文窗口\n",
    "        self.history: List[Dict[str, str]] = []  # 存储对话历史\n",
    "\n",
    "    \"\"\"\n",
    "        添加一轮对话到记忆中，并在必要时截断历史\n",
    "        \"\"\"\n",
    "    def add_conversation(self, user_query: str, agent_response: str) -> None:\n",
    "        self.history.append({\"user\": user_query, \"agent\": agent_response})\n",
    "        self._truncate_history()  # 确保历史不超过最大长度\n",
    "\n",
    "     \"\"\"\n",
    "        截断超长对话历史，从最旧的对话开始删除\n",
    "        确保总长度不超过max_token_len\n",
    "        \"\"\"\n",
    "    def _truncate_history(self) -> None:\n",
    "        while True:\n",
    "            # 1、拼接所有历史对话为文本\n",
    "            history_text = \"\\n\".join([\n",
    "                f\"用户：{item['user']}\\n助手：{item['agent']}\" \n",
    "                for item in self.history\n",
    "            ])\n",
    "            \n",
    "            # 2、计算当前历史的Token长度\n",
    "            token_len = len(self.tokenizer.encode(history_text, add_special_tokens=False))\n",
    "            \n",
    "            # 3、如果长度在限制范围内，或只剩最后一轮对话，则停止截断\n",
    "            if token_len <= self.max_token_len or len(self.history) <= 1:\n",
    "                break\n",
    "            \n",
    "            # 4、删除最旧的一轮对话\n",
    "            self.history.pop(0)\n",
    "        \n",
    "        # 打印当前记忆长度（调试用）\n",
    "        # current_len = len(self.tokenizer.encode(\n",
    "        #     \"\\n\".join([f\"用户：{i['user']}\\n助手：{i['agent']}\" for i in self.history]),\n",
    "        #     add_special_tokens=False\n",
    "        # ))\n",
    "        # print(f\"💾 当前对话记忆长度：{current_len} Token\")\n",
    "        \n",
    "    \"\"\"\n",
    "        生成包含历史对话的提示词，用于模型生成回答\n",
    "        参数:\n",
    "            current_query: 当前用户查询\n",
    "        返回:\n",
    "            包含历史上下文的提示词\n",
    "        \"\"\"\n",
    "    def get_context_prompt(self, current_query: str) -> str:\n",
    "        \n",
    "        # 如果没有历史对话，直接返回当前问题\n",
    "        if not self.history:\n",
    "            return f\"用户问：{current_query}\\n助手回答：\"\n",
    "        \n",
    "        # 拼接历史对话和当前问题\n",
    "        history_text = \"\\n\".join([\n",
    "            f\"用户：{item['user']}\\n助手：{item['agent']}\" \n",
    "            for item in self.history\n",
    "        ])\n",
    "        \n",
    "        # 生成提示词，指导模型基于历史和当前问题生成回答\n",
    "        return f\"\"\"基于以下历史对话和当前问题，生成准确、简洁、有用的回答：\n",
    "{history_text}\n",
    "用户现在问：{current_query}\n",
    "助手回答：\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3926605b",
   "metadata": {},
   "source": [
    "# 4. 决策模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe9ff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def agent_decision(llm_pipeline: pipeline, query: str, memory: ConversationMemory) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Agent决策逻辑：判断是否需要调用工具，以及调用哪个工具\n",
    "    \n",
    "    参数:\n",
    "        llm_pipeline: 文本生成管道\n",
    "        query: 当前用户查询\n",
    "        \n",
    "    返回:\n",
    "        决策结果字典：包含action和可选的tool_name\n",
    "    \"\"\"\n",
    "    # 生成决策提示词\n",
    "    decision_prompt = f\"\"\"\n",
    "任务：判断是否需要调用工具回答以下问题，并选择合适的工具。\n",
    "\n",
    "判断标准：\n",
    "1. 如果是数学计算问题（包含数字和运算符）→ 调用计算器工具，输出：tool:calculator\n",
    "2. 如果是询问本Agent的功能、使用方法 → 调用文档检索工具，输出：tool:document_retrieval\n",
    "3. 如果是需要实时信息的问题（如天气、新闻、实时数据等）→ 调用搜索工具，输出：tool:search\n",
    "4. 其他情况 → 直接回答，输出：direct\n",
    "\n",
    "需要分析的问题：{query}\n",
    "\n",
    "参考历史对话（最近2轮）：{memory.history[-2:] if len(memory.history)>=2 else \"无\"}\n",
    "\n",
    "请严格按照上述格式输出决策结果，仅输出决策内容，不添加额外信息。\n",
    "决策：\n",
    "\"\"\"\n",
    "    \n",
    "    # 调用模型生成决策\n",
    "    decision_result = llm_pipeline(decision_prompt)[0][\"generated_text\"]\n",
    "    # 提取决策结果（取\"决策：\"后面的内容）\n",
    "    decision_result = decision_result.split(\"决策：\")[-1].strip().lower()\n",
    "    \n",
    "    # 解析决策结果\n",
    "    if \"tool:calculator\" in decision_result:\n",
    "        return {\"action\": \"tool\", \"tool_name\": \"calculator\"}\n",
    "    elif \"tool:document_retrieval\" in decision_result:\n",
    "        return {\"action\": \"tool\", \"tool_name\": \"document_retrieval\"}\n",
    "    elif \"tool:search\" in decision_result:\n",
    "        return {\"action\": \"tool\", \"tool_name\": \"search\"}\n",
    "    else:\n",
    "        # 如果决策不明确，默认直接回答\n",
    "        return {\"action\": \"direct\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dff75f",
   "metadata": {},
   "source": [
    "# 5. 工具模块\n",
    "# --------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0ce6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ToolManager:\n",
    "    \"\"\"\n",
    "    工具管理模块，定义Agent可以调用的工具\n",
    "    \n",
    "    目前支持的工具：\n",
    "    - 计算器：处理数学计算\n",
    "    - 文档检索：从本地文档中查找信息\n",
    "    - 模拟搜索：模拟网络搜索功能\n",
    "    \"\"\"\n",
    "    def __init__(self, device: torch.device, doc_path: str = \"agent_knowledge.txt\"):\n",
    "        \"\"\"\n",
    "        初始化工具管理器\n",
    "        \n",
    "        参数:\n",
    "            device: 运行设备\n",
    "            doc_path: 文档检索的知识库路径\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        \n",
    "        # 工具注册表，键为工具名称，值为工具函数\n",
    "        self.tools = {\n",
    "            \"calculator\": self._calculator_tool,\n",
    "            \"document_retrieval\": self._document_retrieval_tool,\n",
    "            \"search\": self._simulate_search_tool\n",
    "        }\n",
    "        \n",
    "        # 初始化文档检索工具\n",
    "        self._init_document_retrieval(doc_path)\n",
    "        print(\"✅ 工具模块初始化完成（支持计算器、文档检索、模拟搜索）\")\n",
    "    \n",
    "    def _init_document_retrieval(self, doc_path: str) -> None:\n",
    "        \"\"\"\n",
    "        初始化文档检索工具（RAG：检索增强生成）\n",
    "        \n",
    "        步骤：\n",
    "        1. 生成样本文档（如果不存在）\n",
    "        2. 加载文档并分割为小块\n",
    "        3. 初始化嵌入模型\n",
    "        4. 创建向量数据库存储文档向量\n",
    "        \"\"\"\n",
    "        # 生成样本文档（Agent的知识库）\n",
    "        if not os.path.exists(doc_path):\n",
    "            with open(doc_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"本Agent基于DeepSeek-R1-Distill-Qwen-14B大语言模型开发，具备以下能力：\\n\")\n",
    "                f.write(\"1. 自然语言对话：可以进行多轮对话，理解上下文语境\\n\")\n",
    "                f.write(\"2. 数学计算：能够处理加减乘除等基本运算，支持括号和小数\\n\")\n",
    "                f.write(\"3. 文档检索：可以回答关于自身功能和使用方法的问题\\n\")\n",
    "                f.write(\"4. 模拟搜索：对于需要实时信息的问题，会提供模拟搜索结果\\n\")\n",
    "                f.write(\"使用方法：直接输入您的问题即可，Agent会自动判断是否需要调用工具来回答\\n\")\n",
    "        \n",
    "        # 1、加载文档\n",
    "        loader = TextLoader(doc_path, encoding=\"utf-8\")\n",
    "        documents = loader.load()\n",
    "\n",
    "        # 将非字符串内容转换为字符串\n",
    "        for i, doc in enumerate(documents):\n",
    "            if not isinstance(doc.page_content, str):\n",
    "                documents[i].page_content = str(doc.page_content)\n",
    "                print(f\"⚠️  修复文档 {i} 内容类型：非字符串→字符串\")\n",
    "        # 2、分割文档为适合嵌入的小块，     递归分割\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=100,      # 每个文本块的大小\n",
    "            chunk_overlap=30,    # 文本块之间的重叠部分\n",
    "            separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]  # 中文分割符优化\n",
    "        )\n",
    "        split_docs = text_splitter.split_documents(documents)\n",
    "        \n",
    "        # 初始化嵌入模型（将文本转换为向量）\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"all-MiniLM-L6-v2\",  # 轻量级嵌入模型\n",
    "            model_kwargs={\"device\": self.device.type}  # 使用GPU加速\n",
    "        )\n",
    "        \n",
    "        # 初始化Chroma向量数据库，存储文档向量\n",
    "        self.vector_db = Chroma.from_documents(\n",
    "            documents=split_docs,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=\"./deepseek_agent_chroma_db\"  # 向量数据持久化路径\n",
    "        )\n",
    "        self.vector_db.persist()  # 保存向量数据库\n",
    "    \n",
    "    def _calculator_tool(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        计算器工具：处理数学计算\n",
    "        \n",
    "        参数:\n",
    "            query: 包含数学表达式的查询\n",
    "            \n",
    "        返回:\n",
    "            计算结果或错误信息\n",
    "        \"\"\"\n",
    "        # 提取查询中的数学表达式\n",
    "        expr_match = re.search(r\"[\\d\\(\\)\\+\\-\\×\\*\\/\\.\\s]+\", query)\n",
    "        if not expr_match:\n",
    "            return \"未检测到有效的数学表达式，请输入包含数字和运算符(+、-、*、/、×、÷)的问题\"\n",
    "        \n",
    "        # 提取并清理表达式\n",
    "        expr = expr_match.group(0).strip()\n",
    "        expr = expr.replace(\"×\", \"*\").replace(\"÷\", \"/\")  # 统一运算符\n",
    "        \n",
    "        # 安全检查：只允许数字、运算符和括号\n",
    "        safe_chars = set(\"0123456789.+-*/() \")\n",
    "        if not all(c in safe_chars for c in expr):\n",
    "            return \"表达式包含不安全字符，仅支持数字和基本运算符(+、-、*、/、()、.)\"\n",
    "        \n",
    "        # 计算表达式\n",
    "        try:\n",
    "            result = eval(expr)  # 使用eval计算表达式\n",
    "            return f\"计算结果：{expr} = {float(result):.4g}\"\n",
    "        except ZeroDivisionError:\n",
    "            return \"计算错误：除数不能为0\"\n",
    "        except Exception as e:\n",
    "            return f\"计算错误：无法计算表达式 '{expr}'，错误信息：{str(e)}\"\n",
    "    \n",
    "    def _document_retrieval_tool(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        文档检索工具：从知识库中检索相关信息\n",
    "        \n",
    "        参数:\n",
    "            query: 检索查询\n",
    "            \n",
    "        返回:\n",
    "            检索到的相关信息\n",
    "        \"\"\"\n",
    "        # 从向量数据库中检索最相关的3个文档片段\n",
    "        retriever = self.vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "        relevant_docs = retriever.get_relevant_documents(query)\n",
    "        \n",
    "        if not relevant_docs:\n",
    "            return \"未检索到相关文档信息\"\n",
    "        \n",
    "        # 拼接检索结果\n",
    "        context = \"\\n\".join([\n",
    "            f\"[相关片段{i+1}] {doc.page_content}\" \n",
    "            for i, doc in enumerate(relevant_docs)\n",
    "        ])\n",
    "        return f\"从知识库中检索到以下相关信息：\\n{context}\"\n",
    "    \n",
    "    def _simulate_search_tool(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        模拟搜索工具：模拟网络搜索功能\n",
    "        \n",
    "        参数:\n",
    "            query: 搜索查询\n",
    "            \n",
    "        返回:\n",
    "            模拟的搜索结果\n",
    "        \"\"\"\n",
    "        return f\"模拟搜索结果（非实时数据）：关于'{query}'的信息\\n\" \\\n",
    "               f\"1. 基本概述：这是一个关于'{query}'的示例搜索结果\\n\" \\\n",
    "               f\"2. 提示：要获取最新信息，请使用浏览器访问搜索引擎\"\n",
    "    \n",
    "    def call_tool(self, tool_name: str, query: str) -> str:\n",
    "        \"\"\"\n",
    "        调用指定工具\n",
    "        \n",
    "        参数:\n",
    "            tool_name: 工具名称\n",
    "            query: 工具输入查询\n",
    "            \n",
    "        返回:\n",
    "            工具执行结果\n",
    "        \"\"\"\n",
    "        if tool_name not in self.tools:\n",
    "            return f\"不支持的工具：{tool_name}，当前支持的工具包括：{list(self.tools.keys())}\"\n",
    "        \n",
    "        try:\n",
    "            return self.tools[tool_name](query)\n",
    "        except Exception as e:\n",
    "            return f\"工具调用出错：{str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f61d33c",
   "metadata": {},
   "source": [
    "# 6. 执行模块\n",
    "# --------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05de774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DeepSeekAgentAssistant:\n",
    "    \"\"\"\n",
    "    基于DeepSeek-R1-Distill-Qwen-14B的Agent助手\n",
    "    \n",
    "    核心功能：\n",
    "    - 接收用户输入\n",
    "    - 决定是否调用工具\n",
    "    - 执行决策（调用工具或直接回答）\n",
    "    - 存储对话历史\n",
    "    - 返回最终回答\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"初始化Agent的所有组件\"\"\"\n",
    "        # 1. 初始化运行环境\n",
    "        self.device = setup_environment()\n",
    "        \n",
    "        # 2. 加载LLM模型和分词器\n",
    "        self.llm_pipeline, self.tokenizer = load_deepseek_llm(self.device)\n",
    "        \n",
    "        # 3. 初始化对话记忆\n",
    "        self.memory = ConversationMemory(self.tokenizer)\n",
    "        \n",
    "        # 4. 初始化工具管理器\n",
    "        self.tool_manager = ToolManager(self.device)\n",
    "        \n",
    "        print(\"🎉 DeepSeek-R1-Distill-Qwen-14B Agent助手初始化完成， ready to use!\")\n",
    "    \n",
    "    def run(self, user_query: str) -> str:\n",
    "        \"\"\"\n",
    "        执行Agent的完整工作流程\n",
    "        \n",
    "        参数:\n",
    "            user_query: 用户输入的查询\n",
    "            \n",
    "        返回:\n",
    "            Agent生成的回答\n",
    "        \"\"\"\n",
    "        print(f\"\\n📥 用户输入：{user_query}\")\n",
    "        \n",
    "        # 步骤1：Agent决策（是否调用工具）\n",
    "        decision = agent_decision(self.llm_pipeline, user_query, self.memory)\n",
    "        print(f\"🔍 Agent决策：{decision}\")\n",
    "        \n",
    "        # 步骤2：执行决策\n",
    "        try:\n",
    "            if decision[\"action\"] == \"tool\":\n",
    "                # 调用工具并获取结果\n",
    "                tool_name = decision[\"tool_name\"]\n",
    "                tool_result = self.tool_manager.call_tool(tool_name, user_query)\n",
    "                print(f\"🛠️  工具执行结果：{tool_result[:200]}...\" if len(tool_result) > 200 else f\"🛠️  工具执行结果：{tool_result}\")\n",
    "                \n",
    "                # 结合工具结果生成最终回答\n",
    "                final_prompt = self.memory.get_context_prompt(user_query)\n",
    "                final_prompt += f\"请根据以下工具结果回答问题：{tool_result}\"\n",
    "                agent_response = self.llm_pipeline(final_prompt)[0][\"generated_text\"]\n",
    "                \n",
    "            else:\n",
    "                # 直接生成回答（不调用工具）\n",
    "                final_prompt = self.memory.get_context_prompt(user_query)\n",
    "                agent_response = self.llm_pipeline(final_prompt)[0][\"generated_text\"]\n",
    "            \n",
    "            # 提取并清理回答（去掉提示词部分和特殊标记）\n",
    "            agent_response = agent_response.split(\"助手回答：\")[-1].strip().replace(\"</s>\", \"\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            # 处理可能的错误\n",
    "            agent_response = f\"处理您的问题时出错：{str(e)}\"\n",
    "            print(f\"❌ 执行错误：{str(e)}\")\n",
    "        \n",
    "        # 步骤3：将本轮对话添加到记忆\n",
    "        self.memory.add_conversation(user_query, agent_response)\n",
    "        \n",
    "        print(f\"📤 Agent回答：{agent_response}\")\n",
    "        return agent_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e6785e",
   "metadata": {},
   "source": [
    "# 7. 测试模块（交互入口）\n",
    "# --------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd2622f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ 注意：使用前请确保已在HuggingFace获取模型访问权限，并通过`huggingface-cli login`命令登录\n",
      "初始化失败：❌ 未检测到可用GPU，DeepSeek-R1-Distill-Qwen-14B需要GPU运行\n",
      "💡 解决方案：\n",
      "1. 确保您的GPU显存不少于16GB\n",
      "2. 确保已安装必要依赖：pip install torch transformers bitsandbytes langchain-huggingface chromadb sentence-transformers\n",
      "3. 确保已获取模型访问权限并成功登录\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 提示用户需要模型访问权限\n",
    "    print(\"⚠️ 注意：使用前请确保已在HuggingFace获取模型访问权限，并通过`huggingface-cli login`命令登录\")\n",
    "    \n",
    "    try:\n",
    "        # 初始化Agent\n",
    "        agent = DeepSeekAgentAssistant()\n",
    "        \n",
    "        # 启动交互循环\n",
    "        print(\"\\n===== DeepSeek-R1-Distill-Qwen-14B Agent助手 =====\")\n",
    "        print(\"提示：输入'退出'可结束对话\")\n",
    "        \n",
    "        while True:\n",
    "            user_input = input(\"你：\")\n",
    "            # 检查是否退出\n",
    "            if user_input.strip().lower() in [\"退出\", \"quit\", \"exit\"]:\n",
    "                print(\"Agent：再见！有任何问题欢迎再次咨询～\")\n",
    "                break\n",
    "            \n",
    "            # 处理用户输入并获取回答\n",
    "            response = agent.run(user_input)\n",
    "            print(f\"Agent：{response}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"初始化失败：{str(e)}\")\n",
    "        print(\"💡 解决方案：\")\n",
    "        print(\"1. 确保您的GPU显存不少于16GB\")\n",
    "        print(\"2. 确保已安装必要依赖：pip install torch transformers bitsandbytes langchain-huggingface chromadb sentence-transformers\")\n",
    "        print(\"3. 确保已获取模型访问权限并成功登录\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9a7f74",
   "metadata": {},
   "source": [
    "# 输出"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
