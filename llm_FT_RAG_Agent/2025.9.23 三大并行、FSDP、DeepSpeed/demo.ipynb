{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b45c6323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import deepspeed\n",
    "import argparse\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    "    set_seed\n",
    ")\n",
    "from transformers.integrations import HfDeepSpeedConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef43be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpi4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38950372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前 pyarrow 版本： 14.0.1\n",
      "是否有 PyExtensionType： True\n"
     ]
    }
   ],
   "source": [
    "import pyarrow\n",
    "print(\"当前 pyarrow 版本：\", pyarrow.__version__)\n",
    "# 检查是否存在 PyExtensionType（正常应输出 <class 'pyarrow._ext.PyExtensionType'>）\n",
    "print(\"是否有 PyExtensionType：\", hasattr(pyarrow, \"PyExtensionType\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3989b59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 环境验证 ===\n",
      "Python 路径： /root/miniconda3/envs/Basic_qwen2\n",
      "transformers 版本： 4.38.0\n",
      "是否支持 'qwen2'： True\n",
      "Qwen2Config 是否存在： True\n"
     ]
    }
   ],
   "source": [
    "# 执行以下代码（复制粘贴）\n",
    "import transformers\n",
    "from transformers.models.auto.configuration_auto import CONFIG_MAPPING\n",
    "\n",
    "print(\"=== 环境验证 ===\")\n",
    "print(\"Python 路径：\", transformers.__file__.split(\"/lib/\")[0])  # 应包含 \"qwen2_env\"\n",
    "print(\"transformers 版本：\", transformers.__version__)  # 必须是 4.38.0\n",
    "print(\"是否支持 'qwen2'：\", \"qwen2\" in CONFIG_MAPPING)  # 必须输出 True\n",
    "print(\"Qwen2Config 是否存在：\", hasattr(transformers.models.qwen2, \"Qwen2Config\"))  # 必须是 True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976ccdff",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# 1. 配置参数解析模块：解析命令行参数和DeepSpeed配置\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e35612cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"DeepSeek-R1-Distill-Qwen-14B 训练脚本（DeepSpeed加速）\")\n",
    "    # 基础训练参数\n",
    "    parser.add_argument(\"--model_name_or_path\", type=str, default=\"./models/DeepSeek-R1-Distill-Qwen-14B\",  help=\"预训练模型路径\")\n",
    "    parser.add_argument(\"--dataset_name\", type=str, default=\"togethercomputer/RedPajama-Data-1T\",  help=\"大规模数据集名称（RedPajama-1T约1TB文本数据）\")\n",
    "    parser.add_argument(\"--dataset_split\", type=str, default=\"train\", help=\"数据集拆分（train/validation）\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"./outputs\", help=\"模型保存路径\")\n",
    "    parser.add_argument(\"--num_train_epochs\", type=int, default=3, help=\"训练轮数\")\n",
    "    parser.add_argument(\"--per_device_train_batch_size\", type=int, default=4, help=\"单GPU训练批次大小\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=2e-5, help=\"学习率\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42, help=\"随机种子\")\n",
    "    \n",
    "    # DeepSpeed参数（通过配置文件指定）\n",
    "    parser.add_argument(\"--deepspeed\", type=str, default=\"ds_config.json\" , help=\"DeepSpeed配置文件路径\")\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"分布式训练本地进程ID（DeepSpeed自动设置）\")\n",
    "    \n",
    "    # 关键修改：用parse_known_args()，忽略Jupyter的--f等未知参数\n",
    "    args, _ = parser.parse_known_args()  # 返回 (已知参数, 未知参数)，用_丢弃未知参数\n",
    "    # 为什么这样改？\n",
    "    # parse_known_args()会只解析你定义过的参数（如--model_name_or_path），自动忽略 Jupyter 传递的--f等未定义参数，彻底解决 “unrecognized arguments” 报错。\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc475d7",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# 2. 数据预处理模块：处理大规模文本数据集\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7680b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_dataset(tokenizer, args):\n",
    "    \"\"\"\n",
    "    加载并预处理RedPajama-1T大规模数据集\n",
    "    RedPajama-1T包含书籍、论文、网页等多源文本，适合语言模型预训练\n",
    "    \"\"\"\n",
    "    # 加载数据集（仅加载必要列，减少内存占用）\n",
    "    dataset = load_dataset(\n",
    "        args.dataset_name,\n",
    "        split=args.dataset_split,\n",
    "        streaming=False,  # 非流式加载（需足够磁盘空间，约1TB）\n",
    "        cache_dir=\"./dataset_cache\"  # 缓存路径，避免重复下载\n",
    "    )\n",
    "    \n",
    "    # 过滤空文本和过短文本\n",
    "    def filter_func(example):\n",
    "        return len(example[\"text\"].strip()) > 100  # 保留长度>100的文本\n",
    "    dataset = dataset.filter(filter_func, num_proc=os.cpu_count())  # 多进程过滤\n",
    "    \n",
    "    # 文本分词与截断\n",
    "    def tokenize_func(example):\n",
    "        # 分词，最大长度512（Qwen模型默认上下文长度）\n",
    "        return tokenizer(\n",
    "            example[\"text\"],\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_overflowing_tokens=False  # 不处理超长文本的截断分片\n",
    "        )\n",
    "    \n",
    "    # 多进程分词（加速处理大规模数据）\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_func,\n",
    "        batched=True,  # 批量处理\n",
    "        num_proc=os.cpu_count(),\n",
    "        remove_columns=[\"text\"]  # 移除原始文本列，节省内存\n",
    "    )\n",
    "    \n",
    "    # 格式化数据集为PyTorch张量\n",
    "    tokenized_dataset = tokenized_dataset.with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    \n",
    "    # 构建标签（自回归任务：标签=输入ID）\n",
    "    tokenized_dataset = tokenized_dataset.map(\n",
    "        lambda x: {\"labels\": x[\"input_ids\"].clone()},\n",
    "        batched=True\n",
    "    )\n",
    "    \n",
    "    return tokenized_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e19db75",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# 3. 模型加载模块：加载预训练模型并配置量化（可选）\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6a34728",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model_and_tokenizer(args):\n",
    "    \"\"\"\n",
    "    加载DeepSeek-R1-Distill-Qwen-14B模型和分词器\n",
    "    支持4/8位量化以节省显存（14B模型全精度约需56GB显存）\n",
    "    \"\"\"\n",
    "    # 加载分词器\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        trust_remote_code=True,  # Qwen系列需要信任远程代码\n",
    "        padding_side=\"left\"  # 自回归模型通常左padding\n",
    "    )\n",
    "    # 设置填充符（如未定义）\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # 配置量化参数（4位量化，适合显存有限的场景）\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,  # 4位量化\n",
    "        bnb_4bit_use_double_quant=True,  # 双量化优化\n",
    "        bnb_4bit_quant_type=\"nf4\",  # 归一化浮点4位\n",
    "        bnb_4bit_compute_dtype=torch.float16  # 计算 dtype\n",
    "    )\n",
    "    \n",
    "    # 加载模型（因果语言模型）\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=bnb_config,  # 应用量化\n",
    "        # device_map=\"auto\",  # 自动分配设备, zero3会自主管理\n",
    "        torch_dtype=torch.float16  # 模型参数 dtype\n",
    "    )\n",
    "    \n",
    "    # 禁用梯度检查点（如需节省显存可开启，但会降低速度）\n",
    "    model.gradient_checkpointing_enable()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec19948",
   "metadata": {},
   "source": [
    "\n",
    "# ==============================================\n",
    "# 4. DeepSpeed训练模块：配置并启动分布式训练\n",
    "# ==============================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "553212e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = parse_args()\n",
    "    set_seed(args.seed)  # 设置随机种子，保证可复现性\n",
    "    # 初始化DeepSpeed配置（必须在模型加载前）\n",
    "    dschf = HfDeepSpeedConfig(args.deepspeed) if args.deepspeed else None\n",
    "    \n",
    "    # 加载模型和分词器\n",
    "    model, tokenizer = load_model_and_tokenizer(args)\n",
    "    \n",
    "    # 准备数据集\n",
    "    tokenized_dataset = prepare_dataset(tokenizer, args)\n",
    "    \n",
    "    # 数据整理器（用于语言模型训练的批量处理）\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False  # 自回归模型不使用掩码语言模型（MLM）\n",
    "    )\n",
    "    \n",
    "    # 配置DeepSpeed训练参数\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "        learning_rate=args.learning_rate,\n",
    "        logging_dir=f\"{args.output_dir}/logs\",\n",
    "        logging_steps=10,  # 每10步记录一次日志\n",
    "        save_steps=100,  # 每100步保存一次模型\n",
    "        save_total_limit=3,  # 最多保存3个模型 checkpoint\n",
    "        deepspeed=args.deepspeed,  # 启用DeepSpeed\n",
    "        local_rank=args.local_rank,  # 分布式训练rank\n",
    "        fp16=True,  # 启用混合精度训练\n",
    "        report_to=\"tensorboard\",  # 日志报告到TensorBoard\n",
    "        remove_unused_columns=False,  # 保留所有列（避免标签被删除）\n",
    "        gradient_accumulation_steps=4,  # 梯度累积（变相增大batch size）\n",
    "        weight_decay=0.01,  # 权重衰减，防止过拟合\n",
    "        warmup_steps=100,  # 学习率热身步数\n",
    "    )\n",
    "    \n",
    "    # 初始化DeepSpeed训练器\n",
    "    model, optimizer, _, _ = deepspeed.initialize(\n",
    "        model=model,\n",
    "        model_parameters=model.parameters(),\n",
    "        config_params=args.deepspeed,\n",
    "        args=training_args,\n",
    "    )\n",
    "    \n",
    "    # ==============================================\n",
    "    # 训练循环\n",
    "    # ==============================================\n",
    "    model.train()\n",
    "    # 生成数据加载器（分布式场景下自动分片）\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=training_args.per_device_train_batch_size,\n",
    "        collate_fn=data_collator,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # 迭代训练\n",
    "    for epoch in range(args.num_train_epochs):\n",
    "        print(f\"===== Epoch {epoch + 1}/{args.num_train_epochs} =====\")\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for step, batch in enumerate(train_loader):\n",
    "            # 数据移动到设备\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(**batch, use_cache=False)  # 禁用缓存以节省显存\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # 反向传播（DeepSpeed自动处理梯度累积和同步）\n",
    "            model.backward(loss)\n",
    "            model.step()  # 优化器更新\n",
    "            \n",
    "            # 记录损失\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # 打印日志\n",
    "            if (step + 1) % training_args.logging_steps == 0:\n",
    "                avg_loss = total_loss / training_args.logging_steps\n",
    "                print(f\"Step {step + 1}, Average Loss: {avg_loss:.4f}\")\n",
    "                total_loss = 0.0\n",
    "            \n",
    "            # 保存模型\n",
    "            if (step + 1) % training_args.save_steps == 0:\n",
    "                model.save_checkpoint(f\"{args.output_dir}/checkpoint-{epoch}-{step}\")\n",
    "        \n",
    "        # 每个epoch结束保存一次模型\n",
    "        model.save_checkpoint(f\"{args.output_dir}/epoch-{epoch + 1}\")\n",
    "    \n",
    "    print(\"训练完成！\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36937c03",
   "metadata": {},
   "source": [
    "# 执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7750778b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 18:11:33,204] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2025-09-23 18:11:33,205] [INFO] [comm.py:616:init_distributed] cdb=None\n",
      "[2025-09-23 18:11:33,206] [INFO] [comm.py:627:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2025-09-23 18:11:34,653] [INFO] [comm.py:677:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.17.0.10, master_port=29500\n",
      "[2025-09-23 18:11:34,654] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2025-09-23 18:11:40,614] [INFO] [partition_parameters.py:326:__exit__] finished initializing model with 3.53B parameters\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m dschf \u001b[38;5;241m=\u001b[39m HfDeepSpeedConfig(args\u001b[38;5;241m.\u001b[39mdeepspeed) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 加载模型和分词器\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_and_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 准备数据集\u001b[39;00m\n\u001b[1;32m     11\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m prepare_dataset(tokenizer, args)\n",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m, in \u001b[0;36mload_model_and_tokenizer\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     17\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m     18\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# 4位量化\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# 双量化优化\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# 归一化浮点4位\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16  \u001b[38;5;66;03m# 计算 dtype\u001b[39;00m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 加载模型（因果语言模型）\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 应用量化\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# device_map=\"auto\",  # 自动分配设备, zero3会自主管理\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 模型参数 dtype\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# 禁用梯度检查点（如需节省显存可开启，但会降低速度）\u001b[39;00m\n\u001b[1;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39mgradient_checkpointing_enable()\n",
      "File \u001b[0;32m~/miniconda3/envs/Basic_qwen2/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/Basic_qwen2/lib/python3.10/site-packages/transformers/modeling_utils.py:3375\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3369\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   3370\u001b[0m     config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[1;32m   3371\u001b[0m )\n\u001b[1;32m   3373\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   3374\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 3375\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3377\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   3378\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "File \u001b[0;32m~/miniconda3/envs/Basic_qwen2/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py:434\u001b[0m, in \u001b[0;36mInsertPostInitMethodToModuleSubClasses.patch_init_and_builtins.<locals>.partition_after.<locals>.wrapper\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m     is_child_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ds_child_entered\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 434\u001b[0m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_child_module:\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;66;03m# child's __init__ is done, now we can run a single post_init on the child object\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28mdelattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ds_child_entered\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/Basic_qwen2/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:1100\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m-> 1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mQwen2Model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/Basic_qwen2/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py:434\u001b[0m, in \u001b[0;36mInsertPostInitMethodToModuleSubClasses.patch_init_and_builtins.<locals>.partition_after.<locals>.wrapper\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m     is_child_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ds_child_entered\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 434\u001b[0m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_child_module:\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;66;03m# child's __init__ is done, now we can run a single post_init on the child object\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28mdelattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ds_child_entered\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/Basic_qwen2/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:932\u001b[0m, in \u001b[0;36mQwen2Model.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m--> 932\u001b[0m     [Qwen2DecoderLayer(config, layer_idx) \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[1;32m    933\u001b[0m )\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_attn_implementation\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m Qwen2RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/miniconda3/envs/Basic_qwen2/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:932\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m--> 932\u001b[0m     [\u001b[43mQwen2DecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[1;32m    933\u001b[0m )\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_attn_implementation\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m Qwen2RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/miniconda3/envs/Basic_qwen2/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py:434\u001b[0m, in \u001b[0;36mInsertPostInitMethodToModuleSubClasses.patch_init_and_builtins.<locals>.partition_after.<locals>.wrapper\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m     is_child_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ds_child_entered\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 434\u001b[0m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_child_module:\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;66;03m# child's __init__ is done, now we can run a single post_init on the child object\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28mdelattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ds_child_entered\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/Basic_qwen2/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:735\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.__init__\u001b[0;34m(self, config, layer_idx)\u001b[0m\n\u001b[1;32m    729\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSliding Window Attention is enabled but not implemented for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    731\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munexpected results may be encountered.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m     )\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m QWEN2_ATTENTION_CLASSES[config\u001b[38;5;241m.\u001b[39m_attn_implementation](config, layer_idx)\n\u001b[0;32m--> 735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m \u001b[43mQwen2MLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm \u001b[38;5;241m=\u001b[39m Qwen2RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm \u001b[38;5;241m=\u001b[39m Qwen2RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/miniconda3/envs/Basic_qwen2/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py:434\u001b[0m, in \u001b[0;36mInsertPostInitMethodToModuleSubClasses.patch_init_and_builtins.<locals>.partition_after.<locals>.wrapper\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m     is_child_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ds_child_entered\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 434\u001b[0m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_child_module:\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;66;03m# child's __init__ is done, now we can run a single post_init on the child object\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28mdelattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ds_child_entered\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/Basic_qwen2/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:181\u001b[0m, in \u001b[0;36mQwen2MLP.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn \u001b[38;5;241m=\u001b[39m ACT2FN[config\u001b[38;5;241m.\u001b[39mhidden_act]\n",
      "File \u001b[0;32m~/miniconda3/envs/Basic_qwen2/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py:441\u001b[0m, in \u001b[0;36mInsertPostInitMethodToModuleSubClasses.patch_init_and_builtins.<locals>.partition_after.<locals>.wrapper\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28mdelattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ds_child_entered\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m     print_rank_0(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRunning post_init for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 441\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post_init_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m print_rank_0(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAfter initializing followed by post init for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/Basic_qwen2/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py:924\u001b[0m, in \u001b[0;36mInit._post_init_method\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m dist\u001b[38;5;241m.\u001b[39mget_rank() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    921\u001b[0m                 logger\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparam `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    922\u001b[0m                             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot on GPU so was not broadcasted from rank 0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 924\u001b[0m         \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m see_memory_usage(\n\u001b[1;32m    926\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParam count \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. After converting and partitioning params in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    927\u001b[0m     force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/Basic_qwen2/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py:1157\u001b[0m, in \u001b[0;36mInit._convert_to_deepspeed_param.<locals>.partition\u001b[0;34m(param_list, backward, hierarchy, has_been_updated)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1156\u001b[0m     param_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m]\n\u001b[0;32m-> 1157\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_partition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_been_updated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_been_updated\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Basic_qwen2/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py:1296\u001b[0m, in \u001b[0;36mInit._partition\u001b[0;34m(self, param_list, force, has_been_updated)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_param_process_group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_partition_param_sec(param, has_been_updated\u001b[38;5;241m=\u001b[39mhas_been_updated)\n\u001b[0;32m-> 1296\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_partition_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_been_updated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_been_updated\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1298\u001b[0m param\u001b[38;5;241m.\u001b[39mds_status \u001b[38;5;241m=\u001b[39m ZeroParamStatus\u001b[38;5;241m.\u001b[39mNOT_AVAILABLE\n",
      "File \u001b[0;32m~/miniconda3/envs/Basic_qwen2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py:15\u001b[0m, in \u001b[0;36minstrument_w_nvtx.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     14\u001b[0m     get_accelerator()\u001b[38;5;241m.\u001b[39mrange_push(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     get_accelerator()\u001b[38;5;241m.\u001b[39mrange_pop()\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret_val\n",
      "File \u001b[0;32m~/miniconda3/envs/Basic_qwen2/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py:1381\u001b[0m, in \u001b[0;36mInit._partition_param\u001b[0;34m(self, param, buffer, has_been_updated)\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;241m<\u001b[39m param\u001b[38;5;241m.\u001b[39mds_numel \u001b[38;5;129;01mand\u001b[39;00m end \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m param\u001b[38;5;241m.\u001b[39mds_numel:\n\u001b[1;32m   1379\u001b[0m     src_tensor \u001b[38;5;241m=\u001b[39m one_dim_param\u001b[38;5;241m.\u001b[39mnarrow(\u001b[38;5;241m0\u001b[39m, start, partition_size)\n\u001b[0;32m-> 1381\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1383\u001b[0m     \u001b[38;5;66;03m#partitioned_tensor = src_tensor.clone().detach().to(self.remote_device)\u001b[39;00m\n\u001b[1;32m   1384\u001b[0m \n\u001b[1;32m   1385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1386\u001b[0m     \u001b[38;5;66;03m# partitioned_tensor = torch.zeros(partition_size,\u001b[39;00m\n\u001b[1;32m   1387\u001b[0m     \u001b[38;5;66;03m#                                  dtype=param.dtype,\u001b[39;00m\n\u001b[1;32m   1388\u001b[0m     \u001b[38;5;66;03m#                                  device=self.remote_device )\u001b[39;00m\n\u001b[1;32m   1390\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;241m<\u001b[39m param\u001b[38;5;241m.\u001b[39mds_numel:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Basic_qwen2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
